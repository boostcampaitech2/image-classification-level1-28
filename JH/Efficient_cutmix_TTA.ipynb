{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cutmix.cutmix import CutMix\n",
    "from cutmix.utils import CutMixCrossEntropyLoss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "from edafa import ClassPredictor\n",
    "import wandb\n",
    "\n",
    "config={\n",
    "    \"lr\": 1e-3,\n",
    "    \"dropout\": 0.5,\n",
    "    \"architecture\": \"efficientnet-b4\",\n",
    "    \"dataset\": \"vanilla + mask\",\n",
    "    \"augmentation\" : \"cutmix\",\n",
    "    \"gamma\" : 0.95,\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 20,\n",
    "    \"memo\" : 'TTA'\n",
    "}\n",
    "config['model'] = '_'.join([config['architecture'], config['augmentation'], str(config['batch_size']), str(config['lr']), str(config['gamma']), config['memo']])\n",
    "# wandb.init(project=\"MaskClassification\", name=config['model'], config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8f33ff-a325-4a5e-95f9-27f0a0cb131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU available : True\n",
      "1 GPU(s) is(are) allocated\n"
     ]
    }
   ],
   "source": [
    "print('CUDA GPU available : {}'.format(torch.cuda.is_available()))\n",
    "try:\n",
    "    print('{} GPU(s) is(are) allocated'.format(torch.cuda.device_count()))\n",
    "except:\n",
    "    print('GPUs are not allocated. Current runtime is on CPU.')\n",
    "device = torch.device(\"cuda\")\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "train_dir = '/opt/ml/input/data/train'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, train_dir, is_Train=True, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        csv_path = os.path.join(train_dir, 'train.csv')\n",
    "        csv = pd.read_csv(csv_path)\n",
    "        self.image_dir = os.path.join(train_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.image_path = []\n",
    "        path = csv['path']\n",
    "        \n",
    "        for p in path:\n",
    "            images = [os.path.join(*[self.image_dir, p, image]) for image in os.listdir(os.path.join(self.image_dir, p)) if not image[:1] == '.']\n",
    "            for image in images:\n",
    "                self.image_path.append(image)\n",
    "                \n",
    "        self.comb_dic = {}\n",
    "        comb = [(m, g, a) for m in ['m', 'i', 'n'] for g in ['male', 'female'] for a in [0, 1, 2]]\n",
    "        for i, (m, g, a) in enumerate(comb):\n",
    "            self.comb_dic[(m, g, a)] = i\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_path[idx]\n",
    "        image = cv2.imread(image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        features = image_name.split('/')[-2:]\n",
    "        \n",
    "        mask = features[1][0]\n",
    "        age = int(features[0].split('_')[-1])\n",
    "        gender = features[0].split('_')[1]\n",
    "        \n",
    "        if age >= 57: # 원래 60\n",
    "            age = 2\n",
    "        elif age >= 30: # 원래 30\n",
    "            age = 1\n",
    "        else:\n",
    "            age = 0        \n",
    "        \n",
    "        target = self.comb_dic[(mask, gender, age)]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.img_paths[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596e6a97-3fe9-458a-9455-ace9d7a99877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "tfms = A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(400, 360, p=1.0),\n",
    "        A.augmentations.geometric.resize.Resize(224, 224, interpolation=1, p=1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.6),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, p=0.6),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.transforms.Normalize(mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246), max_pixel_value=255.0, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2(),\n",
    "    ])\n",
    "tfms_test = A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(400, 360, p=1.0),\n",
    "        A.augmentations.geometric.resize.Resize(224, 224, interpolation=1, p=1),\n",
    "        A.transforms.Normalize(mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246), max_pixel_value=255.0, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2(),\n",
    "    ])\n",
    "val_idx = []\n",
    "person_idx = list(range(2700))\n",
    "\n",
    "np.random.seed(211)\n",
    "np.random.shuffle(person_idx)\n",
    "\n",
    "n_val_person = int(2700 * 0.2)\n",
    "val_person_idx = person_idx[:n_val_person]\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "for i in range(2700*15):\n",
    "    if i // 15 not in val_person_idx:\n",
    "        train_indices.append(i)\n",
    "    else:\n",
    "        val_indices.append(i)\n",
    "\n",
    "# print(train_indices)\n",
    "# print(val_indices)\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# valid_sampler = SubsetRandomSampler(val_indices)\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(val_indices)\n",
    "\n",
    "dataset = TrainDataset(train_dir, transform=tfms)\n",
    "dataset = CutMix(dataset, num_class=18, beta=1.0, prob=0.5, num_mix=2)\n",
    "# print(len(dataset))\n",
    "# plt.imshow(np.array(train_dataset[312][0]['image'].permute(1,2,0)))\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], num_workers=4, pin_memory=True, sampler=train_indices)\n",
    "val_loader = DataLoader(dataset, num_workers=4, batch_size=config['batch_size'], pin_memory=True, sampler=val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeacde3-f357-4129-a73a-2000a48d315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.array(train_dataset[312][0]['image'].permute(1,2,0)))\n",
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21006028-25ad-4414-8bae-730a3bcc33e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3174fa2-72d4-475b-bffd-ad23c4669521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.backbone = EfficientNet.from_pretrained(config['architecture'], num_classes=18)\n",
    "        self.best_f1 = 0\n",
    "        self.best_valid = 999999999\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "        \n",
    "model = Net().to(device)\n",
    "\n",
    "\n",
    "class myPredictor(ClassPredictor):\n",
    "    def __init__(self,model,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.model = model\n",
    "        self.trans = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "    def predict_patches(self,patches):\n",
    "        preds = []\n",
    "        for i in range(patches.shape[0]):\n",
    "            processed = self.trans(patches[i])\n",
    "            pred = self.model(processed)\n",
    "            preds.append(pred.data.numpy())\n",
    "        return self.model(patches)\n",
    "\n",
    "p = myPredictor(model, \"./TTA.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b1aa9f-2fa1-417f-9e3e-7f160fef7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "                             \n",
    "criterion = CutMixCrossEntropyLoss(True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=config['gamma'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d269a9-9771-4364-a927-b06cf5ac86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def check():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    counter = 0\n",
    "    y_true = []\n",
    "    y_predicted = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prog_bar = tqdm(enumerate(val_loader), total=int(len(val_indices)/val_loader.batch_size))\n",
    "        for i, (x, y) in prog_bar:\n",
    "            counter += 1\n",
    "            y_true += y.tolist()\n",
    "            \n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "    \n",
    "            scores = p.predict_images([x])\n",
    "        \n",
    "            \n",
    "            _, predictions = scores.max(1)\n",
    "            y_predicted += predictions.tolist()\n",
    "            \n",
    "            loss = criterion(scores, y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "#     print(y_true, y_predicted)\n",
    "#     cm = confusion_matrix(y_true, y_predicted)\n",
    "#     F1 = []\n",
    "#     for c in range(18):\n",
    "#         precision = cm[c][c] / np.sum(cm, axis=0)[c]\n",
    "#         recall = cm[c][c] / np.sum(cm, axis=1)[c]\n",
    "#         F1.append(2 * precision * recall / (precision + recall))\n",
    "#     macro_F1 = np.mean(F1)\n",
    "    \n",
    "    print(\"< VALIDATION >\")\n",
    "    print(\"*\"*73)\n",
    "    print(\"Validation Loss :\", val_loss/counter)\n",
    "    print(\"-\"*73)\n",
    "#     print(\"Total Accuracy\")\n",
    "#     print(accuracy_score(y_true, y_predicted) * 100, \"%\")\n",
    "#     print(\"-\"*73)\n",
    "#     print(\"Confusion Matrix\")\n",
    "#     for row in cm:\n",
    "#         for c in row:\n",
    "#             print(str(c).ljust(4), end='')\n",
    "#         print()\n",
    "#     print(\"-\"*73)\n",
    "#     print(\"Validation F1 score :\" , macro_F1)\n",
    "#     for c, f in enumerate(F1):\n",
    "#         print(\"Class\", c, \":\", f)\n",
    "        \n",
    "#     if model.best_f1 < macro_F1:\n",
    "#         model.best_f1 = macro_F1\n",
    "    if model.best_valid > val_loss/counter:\n",
    "        model.best_valid = val_loss/counter\n",
    "        torch.save(model.state_dict(), '/opt/ml/weights/{}/{:.4f}.pt'.format(config['model'], val_loss/counter))\n",
    "    print(\"model saved!\")\n",
    "    print(\"*\"*73)\n",
    "    print()\n",
    "    wandb.log({\n",
    "        \"Validation Loss\" : val_loss/counter, \n",
    "#         \"Validation Total Accuracy\" :accuracy_score(y_true, y_predicted) * 100, \n",
    "#         \"Validation F1\" : macro_F1,\n",
    "    })\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c14aa58-ddfa-4ca7-a5d9-3311991b781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/506 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ToTensor() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-64b266c2cc4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/edafa/BasePredictor.py\u001b[0m in \u001b[0;36mpredict_images\u001b[0;34m(self, imgs, overlap)\u001b[0m\n\u001b[1;32m    140\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/edafa/ClassPredictor.py\u001b[0m in \u001b[0;36m_predict_single\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     50\u001b[0m \t\t\"\"\"\n\u001b[1;32m     51\u001b[0m                 \u001b[0maug_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64cd9af7e5b7>\u001b[0m in \u001b[0;36mpredict_patches\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ToTensor() takes no arguments"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "folder = '/opt/ml/weights/{}'.format(config['model'])\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(\"Epoch :\", epoch + 1)\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    total_it = int(len(train_indices)/train_loader.batch_size)\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=total_it)\n",
    "    for i, (inputs, labels) in prog_bar:\n",
    "        \n",
    "        counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        for x in inputs:\n",
    "            x = x.unsqueeze(0).permute(0,2,3,1).detach().cpu().numpy()\n",
    "            output = p.predict_images(x)\n",
    "            print(output)\n",
    "        labels = labels.to(device)\n",
    "        total += labels.size(0)\n",
    "    \n",
    "        #######\n",
    "        preds = []\n",
    "        for img in inputs:\n",
    "            print(img.shape)\n",
    "            img = img[0,:,:,:]\n",
    "            b, h, w, c = img.shape\n",
    "            aug_patch = np.zeros((3,h,w,c),dtype=img.dtype)\n",
    "            for i,aug in enumerate(self.augs):\n",
    "                aug_patch[i] = apply(aug,img,self.bits)\n",
    "            #return aug_patch\n",
    "        print(np.array(preds).shape, np.array(preds))\n",
    "        \n",
    "        #######\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "#         train_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        if i == total_it//2 or i == total_it-1:\n",
    "            train_loss = train_running_loss / counter\n",
    "#             train_accuracy = 100. * train_running_correct / total\n",
    "            \n",
    "            print(\"Train Loss :\", train_loss)\n",
    "#             print(\"Train Accuracy :\", train_accuracy)\n",
    "            wandb.log({\"Train Loss\" : train_loss})\n",
    "            check()\n",
    "    scheduler.step()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf6622-e3d7-401a-84d8-82ba94f6e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/opt/ml/weights/3way/{}/{:.4f}.pt'.format(config['model'], model.best_f1))), model.best_f1\n",
    "model.load_state_dict(torch.load('/opt/ml/weights/{}/0.6784.pt'.format(config['model'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, tfms_test)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=64,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:0\")\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "\n",
    "prog_bar = tqdm(enumerate(loader), total=int(len(dataset)/loader.batch_size))\n",
    "for i, images in prog_bar:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01883cc5-1adf-4c19-ba4b-e53ce43575ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1040\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "image = cv2.imread(image_paths[idx])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "\n",
    "image = tfms_test(image=image)['image'].to(device)\n",
    "image = image.unsqueeze(0)\n",
    "pred = model(image)\n",
    "label = np.argmax(pred.detach().cpu().numpy())\n",
    "#\n",
    "masklabel = {0: \"Mask\", 1: \"Incorrect\", 2: \"Normal\"}\n",
    "genderlabel = {0: \"Male\", 1: \"Female\"}\n",
    "agelabel = {0: \"~ 30\", 1: \"30 ~ 60\", 2: \"60 ~\"}\n",
    "\n",
    "feature_to_label = {}\n",
    "features = [(m, g, a) for m in ['Mask', 'Incorrect', 'Normal'] for g in ['Male', 'Female'] for a in [\"~ 30\", \"30 ~ 60\", \"60 ~\"]]\n",
    "for i, (m, g, a) in enumerate(features):\n",
    "    feature_to_label[(m, g, a)] = i\n",
    "#\n",
    "label_to_feature = { feature_to_label[k]:k for k in feature_to_label}\n",
    "m, g, a = label_to_feature[label]\n",
    "print(m)\n",
    "print(g)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614e887-7750-4c03-80a5-7a0a5b3fab38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4efa0d-f6c8-4bf6-9a0c-8006261bb4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
