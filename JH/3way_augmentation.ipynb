{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoukind\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">efficientnet-b6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/youkind/MaskClassification\" target=\"_blank\">https://wandb.ai/youkind/MaskClassification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/youkind/MaskClassification/runs/sb237zpj\" target=\"_blank\">https://wandb.ai/youkind/MaskClassification/runs/sb237zpj</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/git/JH/wandb/run-20210825_083603-sb237zpj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(sb237zpj)</h1><iframe src=\"https://wandb.ai/youkind/MaskClassification/runs/sb237zpj\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f826b796b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "config={\n",
    "    \"lr\": 2e-4,\n",
    "    \"dropout\": 0.5,\n",
    "    \"architecture\": \"efficientnet-b6 + 3way\",\n",
    "    \"dataset\": \"vanilla + mask\",\n",
    "    \"augmentation\" : \"None\",\n",
    "    \"age_weightedloss\" : [1.0, 1.2, 3.],\n",
    "    \"gender_weightedloss\" : [1.4, 1.0],\n",
    "    \"gamma\" : 0.85,\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 15,\n",
    "    'model' : 'efficientnet-b6'\n",
    "}\n",
    "\n",
    "wandb.init(project=\"MaskClassification\", name=config['model'], config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8f33ff-a325-4a5e-95f9-27f0a0cb131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU available : True\n",
      "1 GPU(s) is(are) allocated\n"
     ]
    }
   ],
   "source": [
    "print('CUDA GPU available : {}'.format(torch.cuda.is_available()))\n",
    "try:\n",
    "    print('{} GPU(s) is(are) allocated'.format(torch.cuda.device_count()))\n",
    "except:\n",
    "    print('GPUs are not allocated. Current runtime is on CPU.')\n",
    "device = torch.device(\"cuda\")\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "\n",
    "labels_to_class = {}\n",
    "it = [(m, g, a) for m in [0,1,2] for g in [0, 1] for a in [0, 1, 2]]\n",
    "for i, (m, g, a) in enumerate(it):\n",
    "    labels_to_class[(m, g, a)] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, train_dir, is_Train=True, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        csv_path = os.path.join(train_dir, 'train.csv')\n",
    "        csv = pd.read_csv(csv_path)\n",
    "        self.image_dir = os.path.join(train_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.image_path = []\n",
    "        path = csv['path']\n",
    "        \n",
    "        for p in path:\n",
    "            images = [os.path.join(*[self.image_dir, p, image]) for image in os.listdir(os.path.join(self.image_dir, p)) if not image[:1] == '.']\n",
    "            for image in images:\n",
    "                self.image_path.append(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_path[idx]\n",
    "        image = cv2.imread(image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        features = image_name.split('/')[-2:]\n",
    "        \n",
    "        masktoint = {'m' : 0, 'i' : 1, 'n' : 2}\n",
    "        gendertoint = {'male' : 0, 'female' : 1}\n",
    "        \n",
    "        mask = masktoint[features[1][0]]\n",
    "        age = int(features[0].split('_')[-1])\n",
    "        gender = gendertoint[features[0].split('_')[1]]\n",
    "        \n",
    "        if age >= 58: # 원래 60\n",
    "            age = 2\n",
    "        elif age >= 30: # 원래 30\n",
    "            age = 1\n",
    "        else:\n",
    "            age = 0        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        return image, (mask, gender, age)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.img_paths[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596e6a97-3fe9-458a-9455-ace9d7a99877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "tfms = A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(400, 360, p=1.0),\n",
    "        A.augmentations.geometric.resize.Resize(224, 224, interpolation=1, p=1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.6),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, p=0.6),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2(),\n",
    "    ])\n",
    "tfms_test = A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(400, 360, p=1.0),\n",
    "        A.augmentations.geometric.resize.Resize(224, 224, interpolation=1, p=1),\n",
    "        A.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "person_idx = list(range(2700))\n",
    "np.random.shuffle(person_idx)\n",
    "n_val_person = int(2700 * 0.2)\n",
    "val_person_idx = person_idx[:n_val_person]\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "for i in range(2700*15):\n",
    "    if i // 15 not in val_person_idx:\n",
    "        train_indices.append(i)\n",
    "    else:\n",
    "        val_indices.append(i)\n",
    "        \n",
    "dataset = TrainDataset(train_dir, transform=tfms)\n",
    "# train_dataset, val_dataset = random_split(dataset, [int(len(dataset)*0.8),int(len(dataset)*0.2)])\n",
    "# print(len(dataset))\n",
    "# plt.imshow(np.array(train[1][0].permute(1,2,0)))\n",
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'], pin_memory=True, num_workers=4, sampler=train_indices)\n",
    "val_loader   = DataLoader(dataset, num_workers=4, batch_size=config['batch_size'], pin_memory=True, sampler=val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d1798-aed1-43e9-b6af-b80be2c27d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeacde3-f357-4129-a73a-2000a48d315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.array(train_dataset[312][0]['image'].permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21006028-25ad-4414-8bae-730a3bcc33e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3174fa2-72d4-475b-bffd-ad23c4669521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b6\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class Way3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.convnet = timm.create_model(config['model'], pretrained=True, num_classes=512)\n",
    "        self.convnet = EfficientNet.from_pretrained(config['model'], num_classes=512).to(device)\n",
    "        self.mask = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=config['dropout'], inplace=False),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=config['dropout'], inplace=False),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.gender = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=config['dropout'], inplace=False),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=config['dropout'], inplace=False),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.age = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=config['dropout'], inplace=False),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=config['dropout'], inplace=False),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.best_f1 = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.convnet(x)\n",
    "        mask = self.mask(features)\n",
    "        gender = self.gender(features)\n",
    "        age = self.age(features)\n",
    "        \n",
    "        return mask, gender, age\n",
    "    \n",
    "model = Way3().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b1aa9f-2fa1-417f-9e3e-7f160fef7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "\n",
    "gender_weight = torch.tensor(config[\"gender_weightedloss\"])\n",
    "age_weight = torch.tensor(config[\"age_weightedloss\"])\n",
    "                             \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "gender_criterion = nn.CrossEntropyLoss(weight=gender_weight).to(device)\n",
    "age_criterion = nn.CrossEntropyLoss(weight=age_weight).to(device)\n",
    "# b_criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=config['gamma'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d269a9-9771-4364-a927-b06cf5ac86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def check(loader, length, model, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    counter = 0\n",
    "    y_true = []\n",
    "    y_predicted = []\n",
    "\n",
    "    m_acc = []\n",
    "    g_acc = []\n",
    "    a_acc = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, (m, g, a)) in val_loader:\n",
    "            counter += 1\n",
    "            \n",
    "            for mask, gender, age in zip(m, g, a):\n",
    "                answer = labels_to_class[(mask.item(), gender.item(), age.item())]\n",
    "                y_true.append(answer)\n",
    "            \n",
    "            inputs = inputs.to(device=device)\n",
    "            m = m.to(device)\n",
    "            g = g.to(device)\n",
    "            a = a.to(device)\n",
    "\n",
    "            m_pred, g_pred, a_pred = model(inputs)\n",
    "    \n",
    "            m_loss = criterion(m_pred, m)\n",
    "            g_loss = gender_criterion(g_pred, g)\n",
    "            a_loss = age_criterion(a_pred, a) # data imbalance\n",
    "            \n",
    "            loss = (g_loss+a_loss+m_loss)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            m_argmax = m_pred.detach().cpu().numpy().argmax(1)\n",
    "            g_argmax = g_pred.detach().cpu().numpy().argmax(1)\n",
    "            a_argmax = a_pred.detach().cpu().numpy().argmax(1)\n",
    "\n",
    "            m_acc.append(accuracy_score(m_argmax, m.detach().cpu().numpy()))\n",
    "            g_acc.append(accuracy_score(g_argmax, g.detach().cpu().numpy()))\n",
    "            a_acc.append(accuracy_score(a_argmax, a.detach().cpu().numpy()))\n",
    "\n",
    "            for mask, gender, age in zip(m_argmax, g_argmax, a_argmax):\n",
    "                predicted = labels_to_class[(mask.item(), gender.item(), age.item())]\n",
    "                y_predicted.append(predicted)\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_predicted)\n",
    "    F1 = []\n",
    "    for c in range(18):\n",
    "        precision = cm[c][c] / np.sum(cm, axis=0)[c]\n",
    "        recall = cm[c][c] / np.sum(cm, axis=1)[c]\n",
    "        F1.append(2 * precision * recall / (precision + recall))\n",
    "    macro_F1 = np.mean(F1)\n",
    "\n",
    "    s = 0\n",
    "    for c in range(18):\n",
    "        s += cm[c][c]\n",
    "        \n",
    "    print(\"< VALIDATION >\")\n",
    "    print(\"*\"*73)\n",
    "    print(\"Validation Loss :\", val_loss/counter)\n",
    "    print(\"-\"*73)\n",
    "    print(\"Total Accuracy\")\n",
    "    print(s / length * 100, \"%\")\n",
    "    print(\"-\"*73)\n",
    "    print(\"Class Accuracy\")\n",
    "    print(\"Mask   :\", np.mean(m_acc)*100, \"%\")\n",
    "    print(\"Gender :\", np.mean(g_acc)*100, \"%\")\n",
    "    print(\"Age    :\", np.mean(a_acc)*100, \"%\")\n",
    "    print(\"-\"*73)\n",
    "    print(\"Confusion Matrix\")\n",
    "    for row in cm:\n",
    "        print(row)\n",
    "    print(\"-\"*73)\n",
    "    print(\"Validation F1 score :\" , macro_F1)\n",
    "    if model.best_f1 < macro_F1:\n",
    "        model.best_f1 = macro_F1\n",
    "        torch.save(model.state_dict(), '/opt/ml/weights/3way/{}/{:.4f}.pt'.format(config['model'], model.best_f1))\n",
    "        print(\"model saved!\")\n",
    "    print(\"*\"*73)\n",
    "    print()\n",
    "    wandb.log({\n",
    "        \"Validation Loss\" : val_loss/counter, \n",
    "        \"Validation Total Accuracy\" : s / length *100, \n",
    "        \"Validation F1\" : macro_F1,\n",
    "        \"Mask Accuracy\" : np.mean(m_acc)*100,\n",
    "        \"Gender Accuracy\" : np.mean(g_acc)*100,\n",
    "        \"Age Accuracy\" : np.mean(a_acc)*100,\n",
    "    })\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c14aa58-ddfa-4ca7-a5d9-3311991b781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 253/506 [03:08<03:06,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 2.210517991246201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a00a7d22706f>:53: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = cm[c][c] / np.sum(cm, axis=0)[c]\n",
      " 50%|█████     | 254/506 [03:33<33:36,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< VALIDATION >\n",
      "*************************************************************************\n",
      "Validation Loss : 2.4499353189168014\n",
      "-------------------------------------------------------------------------\n",
      "Total Accuracy\n",
      "23.703703703703706 %\n",
      "-------------------------------------------------------------------------\n",
      "Class Accuracy\n",
      "Mask   : 97.98501749781278 %\n",
      "Gender : 60.71604330708661 %\n",
      "Age    : 44.82037401574803 %\n",
      "-------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "[  0   0   0 556   0   0   0   0   0   4   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0 319   0   0   0   0   0  11   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0 161   0   0   0   0   0   4   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0 639   0   0   0   0   0   6   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0 712   0   0   0   0   0  18   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0 259   0   0   0   0   0  11   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0   8   0   0   0   0   0 547   0   0   0   0   0   5   0   0]\n",
      "[  0   0   0  23   0   0   0   0   0 307   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0 165   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0   5   0   0   0   0   0 636   0   0   0   0   0   4   0   0]\n",
      "[  0   0   0  41   0   0   0   0   0 684   0   0   0   0   0   5   0   0]\n",
      "[  0   0   0  17   0   0   0   0   0 253   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   1   0   0   0   0   0 559   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 330   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 165   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 645   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 730   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 270   0   0]\n",
      "-------------------------------------------------------------------------\n",
      "Validation F1 score : nan\n",
      "*************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 376/506 [05:03<01:44,  1.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-750e475ad61e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_running_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "folder = '/opt/ml/weights/3way/{}'.format(config['model'])\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(\"Epoch :\", epoch + 1)\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    total_it = int(len(train_indices)/train_loader.batch_size)\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=total_it)\n",
    "    for i, (inputs, (m, g, a)) in prog_bar:\n",
    "        \n",
    "        counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        m_pred, g_pred, a_pred = model(inputs)\n",
    "        \n",
    "        \n",
    "        m = m.to(device)\n",
    "        g = g.to(device)\n",
    "        a = a.to(device)\n",
    "        total += m.size(0)\n",
    "        \n",
    "        \n",
    "        m_loss = criterion(m_pred, m)\n",
    "        g_loss = gender_criterion(g_pred, g)\n",
    "        a_loss = age_criterion(a_pred, a) # data imbalance\n",
    "        \n",
    "        \n",
    "        loss = (g_loss+a_loss+m_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        if i == total_it//2 or i == total_it-1:\n",
    "            train_loss = train_running_loss / counter\n",
    "\n",
    "            print(\"Loss :\", train_loss)\n",
    "            wandb.log({\"Train Loss\" : train_loss})\n",
    "            \n",
    "            check(val_loader, len(val_indices), model, device)\n",
    "    scheduler.step()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf6622-e3d7-401a-84d8-82ba94f6e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/opt/ml/weights/3way/{}/{:.4f}.pt'.format(config['model'], model.best_f1)))\n",
    "# model.load_state_dict(torch.load('/opt/ml/weights/3way/{}/0.9841404744270768.pt'.format(config['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, tfms_test)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=64,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:0\")\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "\n",
    "prog_bar = tqdm(enumerate(loader), total=int(len(dataset)/loader.batch_size))\n",
    "for i, images in prog_bar:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        m_pred, g_pred, a_pred = model(images)\n",
    "#         print(images.shape)\n",
    "#         print(m_pred.shape)\n",
    "        m_argmax = m_pred.detach().cpu().numpy().argmax(1)\n",
    "        g_argmax = g_pred.detach().cpu().numpy().argmax(1)\n",
    "        a_argmax = a_pred.detach().cpu().numpy().argmax(1)\n",
    "        \n",
    "        for mask, gender, age in zip(m_argmax, g_argmax, a_argmax):\n",
    "            predicted = labels_to_class[(mask.item(), gender.item(), age.item())]\n",
    "            all_predictions.append(predicted)\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01883cc5-1adf-4c19-ba4b-e53ce43575ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1020\n",
    "image = cv2.imread(image_paths[idx])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "\n",
    "image = tfms_test(image=image)['image'].to(device)\n",
    "image = image.unsqueeze(0)\n",
    "m_pred, g_pred, a_pred = model(image)\n",
    "\n",
    "m_argmax = m_pred.detach().cpu().numpy().argmax(1)\n",
    "g_argmax = g_pred.detach().cpu().numpy().argmax(1)\n",
    "a_argmax = a_pred.detach().cpu().numpy().argmax(1)\n",
    "\n",
    "#\n",
    "masklabel = {0: \"Mask\", 1: \"Incorrect\", 2: \"Normal\"}\n",
    "genderlabel = {0: \"Male\", 1: \"Female\"}\n",
    "agelabel = {0: \"~ 30\", 1: \"30 ~ 60\", 2: \"60 ~\"}\n",
    "#\n",
    "print(masklabel[m_argmax[0]]+ \" :\", (m_pred[0][m_argmax].item())*100, '%')\n",
    "print(genderlabel[g_argmax[0]]+ \" :\", (g_pred[0][g_argmax].item())*100, '%')\n",
    "print(agelabel[a_argmax[0]]+ \" :\", (a_pred[0][a_argmax].item())*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614e887-7750-4c03-80a5-7a0a5b3fab38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
