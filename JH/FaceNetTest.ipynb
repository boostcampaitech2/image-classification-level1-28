{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c8e09-8042-4224-975b-ddab29d2a2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19bc2c6e-f786-4605-b65f-560fc10b7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import MTCNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b36c19b-00c8-429d-9360-9af9b34fdc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "new_img_dir = '/opt/ml/input/data/train/new_imgs'\n",
    "img_path = '/opt/ml/input/data/train/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e82a9-315d-4742-af3f-dddfc8398ded",
   "metadata": {},
   "source": [
    "## define FaceCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ad9608-6436-452f-9562-4d7b84b20e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import transforms\n",
    "\n",
    "def faceCrop(img, output_img_size : tuple = (224,224)): \n",
    "    '''\n",
    "        image의 얼굴부분만 Crop 하는 함수\n",
    "        img(ndarray) : cv2.imread와 cv2.cvtColor(,cv2.COLOR_BGR2RGB)의 결과값 입력\n",
    "        output_img_size : output으로 받고싶은 image size 입력\n",
    "    '''\n",
    "    \n",
    "    boxes,probs = mtcnn.detect(img) # MTCNN을 활용하여 img내에서 얼굴을 찾음\n",
    "    # 찾은 경우 -> boxes : 좌표값, probs : 확률값 출력\n",
    "    # 못찾은 경우 -> boxes : None, probs : [None] 출력\n",
    "    \n",
    "    if probs[0] == None: # 못찾은 경우, image center 값 출력\n",
    "        print(\"Not Found\")\n",
    "        return img_center_crop(img, output_img_size)\n",
    "    \n",
    "    else :  # 찾은 경우, 얼굴 center기준으로 outputsize를 계산하여 출력\n",
    "        xmid = int((boxes[0, 0]+boxes[0, 2])/2)\n",
    "        ymid = int((boxes[0, 1]+boxes[0, 3])/2)\n",
    "        \n",
    "        xmin = xmid - output_img_size[0]//2\n",
    "        ymin = ymid - output_img_size[1]//2\n",
    "        xmax = xmid + output_img_size[0] - output_img_size[0]//2\n",
    "        ymax = ymid + output_img_size[1] - output_img_size[1]//2\n",
    "        \n",
    "        if any((xmax < 0,xmax > img.shape[1],ymax < 0,ymax > img.shape[0])): # 찾은 후 box가 image 사이즈보다 클려\n",
    "            return img_center_crop(img, output_img_size)\n",
    "        \n",
    "        else :  # \n",
    "            new_img = img[ymin:ymax, xmin:xmax, :]\n",
    "            return new_img\n",
    "\n",
    "def img_center_crop(img, output_img_size : tuple = (224,224)): \n",
    "    ximg_cen = int((img.shape[1])/2)\n",
    "    yimg_cen = int((img.shape[0])/2)\n",
    "\n",
    "    xmin = ximg_cen - output_img_size[0]//2\n",
    "    ymin = yimg_cen - output_img_size[1]//2\n",
    "    xmax = ximg_cen + output_img_size[0] - output_img_size[0]//2\n",
    "    ymax = yimg_cen + output_img_size[1] - output_img_size[1]//2   \n",
    "    \n",
    "    new_img = img[ymin:ymax, xmin:xmax, :]\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46dbb00-02a4-40d9-823a-218f23c2e559",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FaceCrop 함수 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ebd2b6-44f3-4a11-936e-663606912ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask4.jpg\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask1.jpg\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask2.jpg\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask3.jpg\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/normal.jpg\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask.jpg\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask5.jpg\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "new_img_dir = '/opt/ml/input/data/train/new_imgs'\n",
    "img_path = '/opt/ml/input/data/train/images'\n",
    "\n",
    "cnt = 0\n",
    "    \n",
    "sub_dir = os.path.join(img_path, '000001_female_Asian_45')\n",
    "    \n",
    "for img_name in os.listdir(sub_dir):\n",
    "    if img_name[0]=='.': continue\n",
    "    \n",
    "    img_dir = os.path.join(sub_dir, img_name)\n",
    "    img = cv2.imread(img_dir)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    output_size = (224,224)\n",
    "    img = faceCrop(img,output_size) #img input(x,y,3) -> output(224,224,3)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "#     plt.imshow(img)\n",
    "\n",
    "\n",
    "    newpath = os.path.join(new_img_dir, sub_dir)\n",
    "    cnt += 1\n",
    "    print(os.path.join(newpath, img_name))\n",
    "#     plt.imsave(os.path.join(path, img_name), img)\n",
    "               \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82bdb71-8ecd-46e9-8712-43cb21ce91bb",
   "metadata": {},
   "source": [
    "### unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529640d-a07a-4a43-8bb6-55162be85ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = (224,224)\n",
    "\n",
    "img_path = './input/data/train/images'\n",
    "sub_dir = os.path.join(img_path, '006456_female_Asian_18')\n",
    "\n",
    "for img_name in os.listdir(sub_dir):\n",
    "    if img_name[0]=='.': continue\n",
    "    \n",
    "    img_dir = os.path.join(sub_dir, img_name)\n",
    "    img = cv2.imread(img_dir)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#     boxes,probs = mtcnn.detect(img)\n",
    "#     print(img.shape)\n",
    "#     print(boxes,probs)\n",
    "\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2a537-7b83-42bc-8d51-2611af80aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (300,300)\n",
    "\n",
    "img = cv2.imread(\"./input/data/train/images/006456_female_Asian_18/mask4.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# boxes,probs = mtcnn.detect(img)\n",
    "# plt.imshow(img)\n",
    "# print(img.shape)\n",
    "# print(boxes,probs)\n",
    "ximg_cen = int((img.shape[1])/2)\n",
    "yimg_cen = int((img.shape[0])/2)\n",
    "\n",
    "xmin = ximg_cen - input_size[0]//2\n",
    "ymin = yimg_cen - input_size[1]//2\n",
    "xmax = ximg_cen + input_size[0] - input_size[0]//2\n",
    "ymax = yimg_cen + input_size[1] - input_size[1]//2   \n",
    "\n",
    "# new_img = img[ymin:ymax, xmin:xmax, :]\n",
    "\n",
    "# plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2acc3e7-ac61-422b-9cf2-55d2701a7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "any((1,2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d29c77-d3f1-42ac-965d-3ad53d1d63b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_img_size = (224,224)\n",
    "\n",
    "img = cv2.imread(\"./input/data/train/images/006456_female_Asian_18/mask4.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "type(img)\n",
    "# img = img_center_crop(img, input_size)\n",
    "\n",
    "# tran = transforms.Compose([\n",
    "#         transforms.CenterCrop((output_img_size[0]*1.5,output_img_size[1]*1.5)),\n",
    "#         transforms.Resize((output_img_size[0],output_img_size[1])),\n",
    "#         ])\n",
    "# # img = torch.from_numpy(img)\n",
    "# img=transforms.ToPILImage()(img)\n",
    "# tran(img)\n",
    "\n",
    "# tmp = os.path.join(new_img_dir, paths)\n",
    "# cnt += 1\n",
    "# plt.imsave(os.path.join(tmp, 'mask4.jpg'), img)\n",
    "# # plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb12fbf-7f38-4eca-939f-1cf12b58b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tran = transforms.Compose([\n",
    "        transforms.CenterCrop((output_img_size[0]*1.5,output_img_size[1]*1.5)),\n",
    "        transforms.Resize((output_img_size[0],output_img_size[1]))\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
