{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8f33ff-a325-4a5e-95f9-27f0a0cb131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU available : True\n",
      "1 GPU(s) is(are) allocated\n"
     ]
    }
   ],
   "source": [
    "print('CUDA GPU available : {}'.format(torch.cuda.is_available()))\n",
    "try:\n",
    "    print('{} GPU(s) is(are) allocated'.format(torch.cuda.device_count()))\n",
    "except:\n",
    "    print('GPUs are not allocated. Current runtime is on CPU.')\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "train_dir = '/opt/ml/input/data/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, train_dir, is_Train=True, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        csv_path = os.path.join(train_dir, 'train.csv')\n",
    "        csv = pd.read_csv(csv_path)\n",
    "        self.image_dir = os.path.join(train_dir, 'images')\n",
    "        self.transform = transform\n",
    "        self.image_path = []\n",
    "        path = csv['path']\n",
    "        \n",
    "        for p in path:\n",
    "            images = [os.path.join(*[self.image_dir, p, image]) for image in os.listdir(os.path.join(self.image_dir, p)) if not image[:1] == '.']\n",
    "            for image in images:\n",
    "                self.image_path.append(image)\n",
    "                \n",
    "        self.comb_dic = {}\n",
    "        comb = [(m, g, a) for m in ['m', 'i', 'n'] for g in ['male', 'female'] for a in [0, 1, 2]]\n",
    "        for i, (m, g, a) in enumerate(comb):\n",
    "            self.comb_dic[(m, g, a)] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_path[idx]\n",
    "        image = Image.open(image_name)\n",
    "        \n",
    "        features = image_name.split('/')[-2:]\n",
    "        mask = features[1][0]\n",
    "        age = int(features[0].split('_')[-1])\n",
    "        gender = features[0].split('_')[1]\n",
    "        \n",
    "        if age >= 57: # 원래 60\n",
    "            age = 2\n",
    "        elif age >= 26: # 원래 30\n",
    "            age = 1\n",
    "        else:\n",
    "            age = 0        \n",
    "\n",
    "        target = self.comb_dic[(mask, gender, age)]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "596e6a97-3fe9-458a-9455-ace9d7a99877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "tfms = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
    "# tfms = transforms.Compose([\n",
    "# #     transforms.RandomRotation(10),\n",
    "#     transforms.CenterCrop((450, 360)),\n",
    "#     #transforms.Resize((224, 224)), \n",
    "#     transforms.ToTensor(),\n",
    "#     #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "dataset = TrainDataset(train_dir, transform=tfms)\n",
    "train_dataset, val_dataset = random_split(dataset, [int(len(dataset)*0.8),int(len(dataset)*0.2)])\n",
    "# print(len(dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=1, drop_last=False)\n",
    "val_loader   = DataLoader(dataset=val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21006028-25ad-4414-8bae-730a3bcc33e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3174fa2-72d4-475b-bffd-ad23c4669521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b1aa9f-2fa1-417f-9e3e-7f160fef7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "lr = 3e-4\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=0.8, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "    \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d269a9-9771-4364-a927-b06cf5ac86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def check_f1(loader, length, model, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_predicted = []\n",
    "    with torch.no_grad():\n",
    "        prog_bar = tqdm(enumerate(loader), total=int(length/loader.batch_size))\n",
    "        for i, (x, y) in prog_bar:\n",
    "            y_true += y.tolist()\n",
    "\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            y_predicted += predictions.tolist()\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_predicted)\n",
    "    F1 = []\n",
    "    for c in range(18):\n",
    "        precision = cm[c][c] / np.sum(cm, axis=0)[c]\n",
    "        recall = cm[c][c] / np.sum(cm, axis=1)[c]\n",
    "        F1.append(2 * precision * recall / (precision + recall))\n",
    "    macro_F1 = np.mean(F1)\n",
    "    print(\"Confusion Matrix\")\n",
    "    for row in cm:\n",
    "        print(row)\n",
    "    print(\"Validation F1 score :\" , macro_F1)\n",
    "    model.train()\n",
    "\n",
    "def check_accuracy(loader, model, device):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}') \n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9ce6db-56eb-4d50-8793-5caa75b7bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/506 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 18])\n",
      "tensor([15, 17,  5,  4,  4,  4, 17,  3, 15,  4,  6,  3,  0,  9,  9,  5,  4, 10,\n",
      "         5,  9,  9, 13, 16,  9,  3, 14, 13, 16,  2, 10, 10, 10, 16,  4,  3, 12,\n",
      "        12,  1,  0, 13,  9, 11, 15,  0, 10, 17, 17,  2,  7, 10,  9, 15, 14, 12,\n",
      "        15, 15,  2,  5, 15, 17,  0,  3,  5,  6], device='cuda:0')\n",
      "tensor([[ 0.0779,  0.2769, -0.3150,  ...,  0.0709, -0.0359,  0.1472],\n",
      "        [-0.0267,  0.0856,  0.1728,  ..., -0.0639,  0.1057,  0.1735],\n",
      "        [-0.2838,  0.0887, -0.0521,  ..., -0.0049, -0.2390, -0.2795],\n",
      "        ...,\n",
      "        [ 0.3627,  0.2630,  0.2807,  ...,  0.0240,  0.1097,  0.1130],\n",
      "        [-0.3551,  0.2651, -0.0801,  ...,  0.0845,  0.0675,  0.2518],\n",
      "        [ 0.1563, -0.0016, -0.1610,  ...,  0.0270, -0.1215, -0.1958]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/506 [00:01<08:40,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 18])\n",
      "tensor([10, 17, 15,  0,  6, 14,  6,  9,  3, 11,  7,  4, 17,  3,  6,  4, 10,  1,\n",
      "         3, 15,  9,  5,  4,  4, 14, 12,  6, 16, 10,  9,  6,  3,  3, 12,  4, 12,\n",
      "         3, 16,  5, 15,  0, 13, 16,  0,  9, 11,  9, 15,  1, 11,  7, 16,  6, 16,\n",
      "        11, 10,  7,  0,  3,  4,  8, 17, 16,  3], device='cuda:0')\n",
      "tensor([[-0.0445, -0.1637, -0.0078,  ...,  0.1566,  0.1889, -0.0556],\n",
      "        [-0.0301, -0.1406, -0.2427,  ...,  0.1745,  0.1009,  0.2529],\n",
      "        [ 0.0921, -0.0415,  0.0473,  ...,  0.2824,  0.2231, -0.4373],\n",
      "        ...,\n",
      "        [ 0.0131,  0.0085,  0.0316,  ...,  0.3194,  0.1628,  0.0833],\n",
      "        [ 0.2901,  0.2583,  0.6442,  ...,  0.2840, -0.2113,  0.1758],\n",
      "        [-0.0070,  0.0687, -0.0085,  ..., -0.0344, -0.2115,  0.1626]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/506 [00:01<07:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 18])\n",
      "tensor([10,  6, 10,  9, 12, 10,  4, 15,  7,  3, 12,  9,  9,  0,  9,  3,  0, 10,\n",
      "         3,  0, 17,  3,  9,  1, 16,  6, 11, 10, 16, 14, 10, 10, 12,  2, 15, 16,\n",
      "         4, 15, 10,  3, 13,  5, 10, 10, 15,  1,  1,  9, 11,  9,  9, 15, 16, 11,\n",
      "        15,  3,  1, 15, 15, 12,  9, 12, 13,  3], device='cuda:0')\n",
      "tensor([[ 0.1325,  0.1009, -0.2671,  ..., -0.1872,  0.0753, -0.2709],\n",
      "        [ 0.2965,  0.1585, -0.0417,  ...,  0.4029, -0.1907, -0.0940],\n",
      "        [ 0.2387, -0.0300, -0.0659,  ...,  0.1064, -0.0170, -0.1823],\n",
      "        ...,\n",
      "        [-0.2176, -0.1806,  0.0068,  ..., -0.0483,  0.0059, -0.0210],\n",
      "        [-0.1708,  0.0567,  0.0477,  ..., -0.1530,  0.1756, -0.2642],\n",
      "        [ 0.2571,  0.0623,  0.1388,  ...,  0.0446, -0.1363,  0.0916]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/506 [00:02<05:59,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 18])\n",
      "tensor([12, 16,  1, 17, 10, 16, 16, 16,  0,  8, 12, 11, 13, 14, 13, 13, 10, 14,\n",
      "        16,  3, 17,  7,  0,  0, 14,  3, 10, 15,  8, 10,  1,  0,  9,  3, 15, 17,\n",
      "         9, 15,  9, 10,  9,  8, 10, 12, 11,  9, 16, 12, 16, 14, 12,  4,  3,  6,\n",
      "         4,  5, 14,  7,  9,  5,  0, 14,  4,  0], device='cuda:0')\n",
      "tensor([[-0.1385, -0.1266, -0.4030,  ...,  0.3999, -0.0439, -0.0337],\n",
      "        [ 0.2078, -0.1077, -0.2657,  ...,  0.4572, -0.1859, -0.1695],\n",
      "        [-0.0531, -0.0242, -0.0125,  ...,  0.0263, -0.0775, -0.0868],\n",
      "        ...,\n",
      "        [-0.2823,  0.3330, -0.3532,  ..., -0.0620,  0.2539,  0.1009],\n",
      "        [-0.0494, -0.1030, -0.5128,  ..., -0.2821, -0.1620, -0.5189],\n",
      "        [ 0.2025,  0.0563, -0.2505,  ...,  0.0499, -0.3958,  0.0450]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0302f267a02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    total = 0\n",
    "    total_it = int(len(train_dataset)/train_loader.batch_size)\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=total_it)\n",
    "    for i, (inputs, labels) in prog_bar:\n",
    "        \n",
    "        counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        \n",
    "        outputs = model(inputs)\n",
    "#         print(labels.shape)\n",
    "#         print(outputs.shape)\n",
    "#         print(labels)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        if i == total_it//2 or i == total_it-1:\n",
    "            train_loss = train_running_loss / counter\n",
    "            train_accuracy = 100. * train_running_correct / total\n",
    "\n",
    "            print(\"Loss :\", train_loss)\n",
    "            print(\"Accuracy :\", train_accuracy)\n",
    "            print(\"VALIDATION\")\n",
    "            check_f1(val_loader, len(val_dataset), model, device)\n",
    "            print(\"-\"*50)\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, tfms)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:0\")\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "\n",
    "prog_bar = tqdm(enumerate(loader), total=int(len(dataset)/loader.batch_size))\n",
    "for i, images in prog_bar:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd3fb3-9fe2-4305-b8f4-fc97d5058f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb0483-e92d-4af3-b724-4e9968add3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
