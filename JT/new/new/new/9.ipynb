{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djto-8vwvZZ9"
   },
   "source": [
    "## Lesson 9 - Ensemble\n",
    "- 이번 실습 자료에서는 강의시간에 다루었던 Straified kFold Cross Validation(교차 검증)과 이를 활용한 Out-Of-Fold Ensemble 및 Test Time Augmentation에 대해 다뤄보겠습니다. \n",
    "    - k개의 Fold로 나누어 교차 검증하는 kFold Cross Validation은 기존 train, valid 로 나누어 한번만 검증하던 프로세스에서 조금 더 엄격하게 모델을 검증하는 방법입니다. \n",
    "    - kFold Cross Validation에서 학습한 k개의 모델의 결과를 앙상블 하는 것이 Out-Of-Fold(OOF) Ensemble 입니다.\n",
    "    - Test Time Augmentation은 Test 데이터의 결과를 예측할 때 Augmentation 기법을 적용해 여러번 예측하고, 예측한 결과들의 평균으로 결과를 내는 것을 말합니다. \n",
    "- Sklearn의 kFold는 데이터셋의 인덱스로 Fold를 정의하므로 기존에 사용하던 DataSet이 아닌 인덱스로 생성한 Subset을 사용합니다. 이번 실습 자료에서는 매 이터레이션마다 생성한 SubSet 객체를 활용해 DataLoader를 반환해 학습 및 검증에 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgMumhVNvZZ5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# BaseLine 코드로 주어진 dataset.py model.py, loss.py를 Import 합니다.\n",
    "from dataset import MaskBaseDataset, BaseAugmentation\n",
    "from model import *\n",
    "from loss import create_criterion\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tabskC3avZZ-"
   },
   "source": [
    "### Model Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XduaWwdlvZZ-"
   },
   "outputs": [],
   "source": [
    "# -- parameters\n",
    "# img_root = '학습 이미지 폴더의 경로를 입력해주세요.'\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_classes = 18\n",
    "\n",
    "num_epochs = 100  # 학습할 epoch의 수\n",
    "lr = 1e-4\n",
    "lr_decay_step = 10\n",
    "criterion_name = 'cross_entropy' # loss의 이름\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_model_results\"  # 결과를 저장하는 폴더의 이름\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jONusDlUvZZ-"
   },
   "source": [
    "### DataLoader\n",
    "- index를 사용한 Dataloader 정의\n",
    "- getDataloader 함수 설명\n",
    "    1. Pytorch Dataset, train 인덱스, valid 인덱스, batch size를 전달받아 Train, Valid DataLoader 객체를 반환합니다.\n",
    "    2. torch.utils.data.Subset 객체는 데이터셋과 해당 데이터셋의 인덱스를 전달받아 Subset 객체를 생성합니다. 생성한 Subset 객체를 사용해 DataLoader 객체를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaHofXguvZZ_"
   },
   "outputs": [],
   "source": [
    "def getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers):\n",
    "    # 인자로 전달받은 dataset에서 train_idx에 해당하는 Subset 추출\n",
    "    train_set = torch.utils.data.Subset(dataset,\n",
    "                                        indices=train_idx)\n",
    "    # 인자로 전달받은 dataset에서 valid_idx에 해당하는 Subset 추출\n",
    "    val_set   = torch.utils.data.Subset(dataset,\n",
    "                                        indices=valid_idx)\n",
    "    \n",
    "    # 추출된 Train Subset으로 DataLoader 생성\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # 추출된 Valid Subset으로 DataLoader 생성\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 생성한 DataLoader 반환\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkUoWwJnvZZ_"
   },
   "source": [
    "### Stratified k-Fold\n",
    "1. k를 나타내는 n_splits 값을 설정해 StratifiedKFold 객체를 준비합니다. \n",
    "2. skf.split(x, y) 메소드를 사용해 데이터셋의 인덱스를 얻어내고, 라벨(y)을 기준으로 Stratified를 진행합니다. \n",
    "3. 매 이터레이션마다 반환받은 인덱스를 사용해 getDataloader 함수에 전달하여 DataLoader를 생성합니다.\n",
    "4. 나머지 학습 프로세스는 이전 강의에서 사용한 프로세스와 동일하게 진행합니다.\n",
    "- Stratify는 데이터셋을 분리하기 이전에 ```클래스 비율```을 분리한 이후에도 유지해주는 기능을 말합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExHJ2_qvvZZ_"
   },
   "outputs": [],
   "source": [
    "dataset = MaskBaseDataset(img_root)\n",
    "\n",
    "transform = BaseAugmentation(\n",
    "    resize=[128, 96],\n",
    "    mean=dataset.mean,\n",
    "    std=dataset.std,\n",
    ")\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpsrKzcuvZZ_"
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "# 5-fold Stratified KFold 5개의 fold를 형성하고 5번 Cross Validation을 진행합니다.\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "labels = [dataset.encode_multi_class(mask, gender, age) for mask, gender, age in zip(dataset.mask_labels, dataset.gender_labels, dataset.age_labels)]\n",
    "\n",
    "# Stratified KFold를 사용해 Train, Valid fold의 Index를 생성합니다.\n",
    "# labels 변수에 담긴 클래스를 기준으로 Stratify를 진행합니다. \n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(dataset.image_paths, labels)):\n",
    "    \n",
    "    # 생성한 Train, Valid Index를 getDataloader 함수에 전달해 train/valid DataLoader를 생성합니다.\n",
    "    # 생성한 train, valid DataLoader로 이전과 같이 모델 학습을 진행합니다. \n",
    "    train_loader, val_loader = getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers)\n",
    "\n",
    "    # -- model\n",
    "    model = BaseModel(num_classes=num_classes).to(device)\n",
    "\n",
    "    # -- loss & metric\n",
    "    criterion = create_criterion(criterion_name)\n",
    "    train_params = [{'params': getattr(model.net, 'features').parameters(), 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                    {'params': getattr(model.net, 'classifier').parameters(), 'lr': lr, 'weight_decay':5e-4}]\n",
    "    optimizer = Adam(train_params)\n",
    "    scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    logger = SummaryWriter(log_dir=f\"results/cv{i}_{name}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "             # -- Gradient Accumulation\n",
    "            if (idx+1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % train_log_interval == 0:\n",
    "                train_loss = loss_value / train_log_interval\n",
    "                train_acc = matches / batch_size / train_log_interval\n",
    "                current_lr = scheduler.get_last_lr()\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            for val_batch in val_loader:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outs = model(inputs)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(valid_idx)\n",
    "\n",
    "            # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\"New best model for val accuracy! saving the model..\")\n",
    "                torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "            if counter > patience:\n",
    "                print(\"Early Stopping...\")\n",
    "                break\n",
    "\n",
    "\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee1GXk-qvZaA"
   },
   "source": [
    "### Out-Of-Fold Ensemble with TTA\n",
    "1. 이전 단계에서 작성한 k-Fold Cross Validation 코드에 각 폴드들의 예측 결과를 담을 oof_pred 변수를 준비합니다.\n",
    "    - 생성한 행렬의 차원은 (Test set의 샘플 수, 예측할 클래스의 개수)로 정의됩니다. \n",
    "    - 각 열에는 주어진 이미지가 해당 클래스일 확률을 담습니다. (Soft Voting), 여기에서 확률이 아닌 라벨로 앙상블을 진행하는 경우를 Hard Voting이라 합니다.\n",
    "    - 확률을 저장할 때는 모든 확률의 합은 1이라는 확률의 정의를 유지하기 위해 확률을 k로 나누어 평균을 취해줍니다.\n",
    "2. 각 행에 대해 가장 높은 확률을 갖는 클래스를 선택해 결과를 도출합니다.\n",
    "3. 테스트 셋을 예측할 때 Augmentation 기법을 적용하여 Test Time Augmentation을 진행합니다. \n",
    "    - 해당 실습 코드에서는 flip을 적용하여 2번 TTA를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpv0075KvZaB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "test_img_root = '학습 이미지 폴더의 경로' \n",
    "# public, private 테스트셋이 존재하니 각각의 예측결과를 저장합니다.\n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_img_root, 'info.csv'))\n",
    "image_dir = os.path.join(test_img_root, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((128, 96), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)),\n",
    "])\n",
    "test_dataset = TestDataset(test_img_root, transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTKpGyHovZaB"
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "oof_pred = None\n",
    "\n",
    "labels = [dataset.encode_multi_class(mask, gender, age) for mask, gender, age in zip(dataset.mask_labels, dataset.gender_labels, dataset.age_labels)]\n",
    "\n",
    "# K-Fold Cross Validation과 동일하게 Train, Valid Index를 생성합니다. \n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(dataset.image_paths, labels)):\n",
    "    train_loader, val_loader = getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers)\n",
    "\n",
    "    # -- model\n",
    "    model = BaseModel(num_classes=num_classes).to(device)\n",
    "\n",
    "    # -- loss & metric\n",
    "    criterion = create_criterion(criterion_name)\n",
    "    train_params = [{'params': getattr(model.net, 'features').parameters(), 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                    {'params': getattr(model.net, 'classifier').parameters(), 'lr': lr, 'weight_decay':5e-4}]\n",
    "    optimizer = Adam(train_params)\n",
    "    scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    logger = SummaryWriter(log_dir=f\"results/cv{i}_{name}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "             # -- Gradient Accumulation\n",
    "            if (idx+1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % train_log_interval == 0:\n",
    "                train_loss = loss_value / train_log_interval\n",
    "                train_acc = matches / batch_size / train_log_interval\n",
    "                current_lr = scheduler.get_last_lr()\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            for val_batch in val_loader:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outs = model(inputs)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(valid_idx)\n",
    "\n",
    "            # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\"New best model for val accuracy! saving the model..\")\n",
    "                torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "            if counter > patience:\n",
    "                print(\"Early Stopping...\")\n",
    "                break\n",
    "\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )\n",
    "            \n",
    "    # 각 fold에서 생성된 모델을 사용해 Test 데이터를 예측합니다. \n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Test Time Augmentation\n",
    "            pred = model(images) / 2 # 원본 이미지를 예측하고\n",
    "            pred += model(torch.flip(images)) / 2 # flip으로 뒤집어 예측합니다. \n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "        fold_pred = np.array(all_predictions)\n",
    "\n",
    "    # 확률 값으로 앙상블을 진행하기 때문에 'k'개로 나누어줍니다.\n",
    "    if oof_pred is None:\n",
    "        oof_pred = fold_pred / n_splits\n",
    "    else:\n",
    "        oof_pred += fold_pred / n_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_wCWSL-vZaC"
   },
   "source": [
    "### Submission 파일 만들기\n",
    "- 각 fold에서 예측한 결과의 확률을 평균낸 oof_pred에서 가장 높은 확률을 갖는 클래스를 추출하고 csv 파일을 생성합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lGYbEwAvZaC"
   },
   "outputs": [],
   "source": [
    "submission['ans'] = np.argmax(oof_pred, axis=1)\n",
    "submission.to_csv(os.path.join(test_img_root, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_Ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
