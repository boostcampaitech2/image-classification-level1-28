{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nTWu0GL8Ifz"
   },
   "source": [
    "## Lesson 10 - Experiment Toolkit\n",
    "- 이번 실습 자료에서는 강의시간에 다루었던 Tensorboard & wandb 연동 코드에 대해 실습을 진행합니다.\n",
    "    1. 실습을 진행하기 앞서 먼저 'pip install wandb'를 통해 wandb를 설치하시고, 'wandb.ai' 홈페이지에서 회원가입을 진행 합니다.\n",
    "    2. 터미널 창에서 wandb login 을 입력하여 사용자 계정과 로컬 환경을 연결합니다.\n",
    "        - 사용자 계정은 wandb.ai 홈페이지에서 로그인 후 profile - Settings - API Keys 항목에서 API키를 생성하여 연결할 수 있습니다. 최초 한번 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmiNC_MY8Ifv"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# BaseLine 코드로 주어진 dataset.py model.py, loss.py를 Import 합니다.\n",
    "from dataset import MaskBaseDataset, BaseAugmentation\n",
    "from model import *\n",
    "from loss import create_criterion\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import wandb\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajpxZtFu8If0"
   },
   "source": [
    "### Model Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veN3QHOc8If0"
   },
   "outputs": [],
   "source": [
    "# -- parameters\n",
    "# img_root = '학습 이미지 폴더의 경로를 입력해주세요.'\n",
    "\n",
    "val_split = 0.4  # validation dataset의 비율\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_classes = 18\n",
    "\n",
    "num_epochs = 100  # 학습할 epoch의 수\n",
    "lr = 1e-4\n",
    "lr_decay_step = 10\n",
    "criterion_name = 'cross_entropy'\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_model_results\"  # 결과를 저장하는 폴더의 이름\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPjsk40V8If1"
   },
   "source": [
    "### wandb init\n",
    "- wandb를 사용하기 앞서 먼저 초기화를 진행하는데 이 때, 모델 학습에 사용할 파라미터를 같이 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzwjAzWC8If2"
   },
   "outputs": [],
   "source": [
    "# -- wandb initialize with configuration\n",
    "wandb.init(config={\"batch_size\": batch_size,\n",
    "                   \"lr\"        : lr,\n",
    "                   \"epochs\"    : num_epochs,\n",
    "                   \"backborn\"  : model_name,\n",
    "                   \"criterion_name\" : criterion_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8n_d4CX8If2"
   },
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYncGKTn8If2"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = MaskBaseDataset(img_root)\n",
    "\n",
    "# Augmentation Transform 생성\n",
    "transform = BaseAugmentation(\n",
    "    resize=[128, 96],\n",
    "    mean=dataset.mean,\n",
    "    std=dataset.std,\n",
    ")\n",
    "\n",
    "# 데이터셋 준비\n",
    "dataset.set_transform(transform)\n",
    "n_val = int(len(dataset) * val_split)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Train Valid DataLoader 생성\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# -- model\n",
    "model = BaseModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# -- loss & metric\n",
    "criterion = create_criterion(criterion_name)\n",
    "train_params = [{'params': getattr(model.net, 'features').parameters(), 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                {'params': getattr(model.net, 'classifier').parameters(), 'lr': lr, 'weight_decay':5e-4}]\n",
    "optimizer = Adam(train_params)\n",
    "scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2X1yK_48If2"
   },
   "source": [
    "### Tensorboard\n",
    "- Tensorboard는 먼저 SummaryWriter 객체에 log_dir 인자(로그를 저장할 디렉토리 경로)를 전달하여 로그를 저장할 준비를 합니다. \n",
    "    - 특정 주기마다 logger.add_scaler(이름, 값, 글로벌 스텝)을 전달하여 스칼라 값 로그를 기록합니다. \n",
    "    - 특정 주기마다 logger.add_image(이미지 그리드)를 전달하여 이미지 로그를 기록합니다. \n",
    "        - 여기에서는 train step에서 각 이미지들이 어떤식으로 Transform 되는지 기록해보겠습니다.\n",
    "- 터미널에서 tensorboard --logdir='로그를 저장한 디렉토리 경로' 를 입력하여 텐서보드를 실행해 로그 기록을 확인할 수 있습니다.\n",
    "\n",
    "### wandb\n",
    "- wandb는 이전에 init 함수를 통해 초기화를 마쳤으므로 특정 주기마다 wandb.log({이름: 값, ...})를 전달하여 로그를 기록합니다.\n",
    "- wandb.ai 홈페이지에서 로그 기록을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDc0B9RS8If3"
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "# Tensorboard 로그를 저장할 경로 지정\n",
    "logger = SummaryWriter(log_dir=f\"logdir/{name}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    \n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # -- Gradient Accumulation\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % train_log_interval == 0:\n",
    "            train_loss = loss_value / train_log_interval\n",
    "            train_acc = matches / batch_size / train_log_interval\n",
    "            current_lr = scheduler.get_last_lr()\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "            \n",
    "            # Tensorboard 학습 단계에서 Loss, Accuracy 로그 저장\n",
    "            logger.add_scalar(\"Train/loss\", train_loss, epoch * len(train_loader) + idx)\n",
    "            logger.add_scalar(\"Train/accuracy\", train_acc, epoch * len(train_loader) + idx)\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "            \n",
    "            # wandb 학습 단계에서 Loss, Accuracy 로그 저장\n",
    "            wandb.log({\n",
    "                \"Train loss\": train_loss,\n",
    "                \"Train acc\" : train_acc\n",
    "            })\n",
    "    \n",
    "    # 각 에폭의 마지막 input 이미지로 grid view 생성\n",
    "    img_grid = torchvision.utils.make_grid(inputs)\n",
    "    # Tensorboard에 train input 이미지 기록\n",
    "    logger.add_image(f'{epoch}_train_input_img', img_grid, epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(val_set)\n",
    "        \n",
    "        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy! saving the model..\")\n",
    "            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "        \n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        # Tensorboard 검증 단계에서 Loss, Accuracy 로그 저장\n",
    "        logger.add_scalar(\"Val/loss\", val_loss, epoch)\n",
    "        logger.add_scalar(\"Val/accuracy\", val_acc, epoch)\n",
    "\n",
    "        # wandb 검증 단계에서 Loss, Accuracy 로그 저장\n",
    "        wandb.log({\n",
    "            \"Valid loss\": val_loss,\n",
    "            \"Valid acc\" : val_acc\n",
    "        })\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8_Experiment Toolkit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
