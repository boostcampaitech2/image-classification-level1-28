{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from facenet_pytorch import MTCNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "new_img_dir = '../input/data/train/new_imgs'\n",
    "img_path = './input/data/train/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define FaceCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import transforms\n",
    "\n",
    "def faceCrop(img, extend_size = 50, output_img_size : tuple = (310,310)): \n",
    "    '''\n",
    "        image의 얼굴부분만 Crop 하는 함수\n",
    "        img(ndarray) : cv2.imread와 cv2.cvtColor(,cv2.COLOR_BGR2RGB)의 결과값 입력\n",
    "        output_img_size : output으로 받고싶은 image size 입력\n",
    "    '''\n",
    "    \n",
    "    boxes,probs = mtcnn.detect(img) # MTCNN을 활용하여 img내에서 얼굴을 찾음\n",
    "                                    # 찾은 경우 -> boxes : 좌표값, probs : 확률값 출력\n",
    "                                    # 못찾은 경우 -> boxes : None, probs : [None] 출력\n",
    "    \n",
    "    if probs[0] == None: # 못찾은 경우, image center 값 반환\n",
    "        return img_center_crop(img, output_img_size)\n",
    "    \n",
    "    else :  # 찾은 경우, 얼굴 center기준으로 outputsize를 계산하여 반환    \n",
    "        xmid = int((boxes[0, 0]+boxes[0, 2])/2)\n",
    "        ymid = int((boxes[0, 1]+boxes[0, 3])/2)\n",
    "\n",
    "        xmin = xmid - output_img_size[0]//2\n",
    "        ymin = ymid - output_img_size[1]//2\n",
    "        xmax = xmid + output_img_size[0] - output_img_size[0]//2\n",
    "        ymax = ymid + output_img_size[1] - output_img_size[1]//2\n",
    "        \n",
    "        if any((xmin < 0,xmax > img.shape[1],ymin < 0,ymax > img.shape[0])): # 찾은 후 box가 image 사이즈보다 클 경우 center 위치 반환\n",
    "            return img_center_crop(img, output_img_size)\n",
    "        \n",
    "        else :  #정상적으로 얼굴만 crop된 image 반환\n",
    "            new_img = img[ymin:ymax, xmin:xmax, :]\n",
    "            return new_img\n",
    "\n",
    "def img_center_crop(img, output_img_size : tuple = (100,100)): #image 내에서 얼굴을 못찾거나, image를 벗어난 경우 center crop 하는 함수\n",
    "    ximg_cen = int((img.shape[1])/2)\n",
    "    yimg_cen = int((img.shape[0])/2)\n",
    "\n",
    "    xmin = ximg_cen - output_img_size[0]//2\n",
    "    ymin = yimg_cen - output_img_size[1]//2\n",
    "    xmax = ximg_cen + output_img_size[0] - output_img_size[0]//2\n",
    "    ymax = yimg_cen + output_img_size[1] - output_img_size[1]//2   \n",
    "    new_img = img[ymin:ymax, xmin:xmax, :]\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FaceCrop 함수 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "new_img_dir = './input/data/train/new_imgs'\n",
    "img_path = './input/data/train/images'\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "# for paths in tqdm(os.listdir(img_path)):\n",
    "#     if paths[0] == '.': continue\n",
    "    \n",
    "sub_dir = os.path.join(img_path, '006456_female_Asian_18')\n",
    "\n",
    "for img_name in os.listdir(sub_dir):\n",
    "    if img_name[0]=='.': continue\n",
    "\n",
    "    img_dir = os.path.join(sub_dir, img_name)\n",
    "    img = cv2.imread(img_dir)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    output_size = (224,224)\n",
    "    img = faceCrop(img,output_size) #img input(x,y,3) -> output(224,224,3)\n",
    "\n",
    "    tmp = os.path.join(new_img_dir, paths)\n",
    "    cnt += 1\n",
    "    plt.imsave(os.path.join(tmp, img_name), img)\n",
    "               \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = (224,224)\n",
    "\n",
    "img_path = './input/data/train/images'\n",
    "sub_dir = os.path.join(img_path, '006456_female_Asian_18')\n",
    "\n",
    "for img_name in os.listdir(sub_dir):\n",
    "    if img_name[0]=='.': continue\n",
    "    \n",
    "    img_dir = os.path.join(sub_dir, img_name)\n",
    "    img = cv2.imread(img_dir)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#     boxes,probs = mtcnn.detect(img)\n",
    "#     print(img.shape)\n",
    "#     print(boxes,probs)\n",
    "\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (300,300)\n",
    "\n",
    "img = cv2.imread(\"./input/data/train/images/006456_female_Asian_18/mask4.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# boxes,probs = mtcnn.detect(img)\n",
    "# plt.imshow(img)\n",
    "# print(img.shape)\n",
    "# print(boxes,probs)\n",
    "ximg_cen = int((img.shape[1])/2)\n",
    "yimg_cen = int((img.shape[0])/2)\n",
    "\n",
    "xmin = ximg_cen - input_size[0]//2\n",
    "ymin = yimg_cen - input_size[1]//2\n",
    "xmax = ximg_cen + input_size[0] - input_size[0]//2\n",
    "ymax = yimg_cen + input_size[1] - input_size[1]//2   \n",
    "\n",
    "# new_img = img[ymin:ymax, xmin:xmax, :]\n",
    "\n",
    "# plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any((1,2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_img_size = (224,224)\n",
    "\n",
    "img = cv2.imread(\"./input/data/train/images/006456_female_Asian_18/mask4.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "type(img)\n",
    "# img = img_center_crop(img, input_size)\n",
    "\n",
    "# tran = transforms.Compose([\n",
    "#         transforms.CenterCrop((output_img_size[0]*1.5,output_img_size[1]*1.5)),\n",
    "#         transforms.Resize((output_img_size[0],output_img_size[1])),\n",
    "#         ])\n",
    "# # img = torch.from_numpy(img)\n",
    "# img=transforms.ToPILImage()(img)\n",
    "# tran(img)\n",
    "\n",
    "# tmp = os.path.join(new_img_dir, paths)\n",
    "# cnt += 1\n",
    "# plt.imsave(os.path.join(tmp, 'mask4.jpg'), img)\n",
    "# # plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tran = transforms.Compose([\n",
    "        transforms.CenterCrop((output_img_size[0]*1.5,output_img_size[1]*1.5)),\n",
    "        transforms.Resize((output_img_size[0],output_img_size[1]))\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
