{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e74c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU availalbe : True\n",
      "1 GPU(s) is(are) allocated\n"
     ]
    }
   ],
   "source": [
    "# GPU 할당확인 \n",
    "import torch\n",
    "\n",
    "print('CUDA GPU availalbe : {}'.format(torch.cuda.is_available()))\n",
    "try:\n",
    "    print('{} GPU(s) is(are) allocated'.format(torch.cuda.device_count()))\n",
    "except:\n",
    "    print('GPUs are not allocated. Current runtime is on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541b42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb1a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "\n",
    "\n",
    "# config = {\"epochs\":epochs, \"batch_size\":batch_size, \"learning_rate\":lr}\n",
    "# wandb.init(project=\"boost_2nd_pstage_1\",config=config)\n",
    "csv_dir = 'input/data/train/train.csv'\n",
    "train_dir = 'input/data/train'\n",
    "root_dir = 'input/data/train/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac505ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "lr = 3e-4\n",
    "model_name = \"Augment_1\"\n",
    "seed = 211\n",
    "val_ratio = 0.2\n",
    "check_period = 40\n",
    "fullTrain = True\n",
    "\n",
    "csv_dir = 'input/data/train/train.csv'\n",
    "train_dir = 'input/data/train'\n",
    "root_dir = 'input/data/train/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e4dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, is_Train=True, val_ratio = 0.2, seed = 42, full_Train = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "            img_dir (string): 모든 이미지가 존재하는 디렉토리 경로\n",
    "            transform (callable, optional): 샘플에 적용될 Optional transform\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_path = []\n",
    "\n",
    "        if not full_Train:\n",
    "            path = csv.path.values\n",
    "            np.random.seed(seed) \n",
    "            np.random.shuffle(path)\n",
    "            n_val = int(len(path) * val_ratio)\n",
    "            \n",
    "            if is_Train:\n",
    "                people = path[n_val:]\n",
    "            else:\n",
    "                people = path[:n_val]\n",
    "        else:\n",
    "            people = csv.path.values\n",
    "\n",
    "        for person in people:\n",
    "            images = [root_dir + person + '/' + image for image in os.listdir(root_dir + person) if not image[:1] == '.']\n",
    "            for image in images:\n",
    "                self.img_path.append(image)\n",
    "\n",
    "        self.comb_dic = {}\n",
    "        comb = [(m, g, a) for m in ['m', 'i', 'n'] for g in ['male', 'female'] for a in [0, 1, 2]]\n",
    "        for i, (m, g, a) in enumerate(comb):\n",
    "            self.comb_dic[(m, g, a)] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_path[idx]\n",
    "\n",
    "        mask = img_name.split('/')[-1][:1]\n",
    "        features = img_name.split('/')[-2].split('_')\n",
    "        gender = features[1]\n",
    "        if features[3] in ['58', '59']:\n",
    "            age = 2\n",
    "        else:\n",
    "            age = int(features[3]) // 30\n",
    "        target = self.comb_dic[(mask, gender, age)]\n",
    "            \n",
    "        # Albumentations\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "        # Albumentations\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d44faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=18)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cd57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5385d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataset import random_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    A.RandomCrop(height = 450, width = 360),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "#     A.CenterCrop(height=450, width=360),\n",
    "    A.SmallestMaxSize(max_size=160),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "transform_val = A.Compose([\n",
    "#     A.ShiftScaleRotate(p=0.5),\n",
    "    A.CenterCrop(height=450, width=360),\n",
    "    A.SmallestMaxSize(max_size=160),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation set generate\n",
    "if not fullTrain:\n",
    "    train_dataset = MyDataset(csv_dir, root_dir, is_Train=True, val_ratio = val_ratio, seed = seed, \n",
    "                             full_Train = fullTrain, transform=transform_train)\n",
    "    val_dataset = MyDataset(csv_dir, root_dir, is_Train=False, val_ratio = val_ratio, seed = seed, \n",
    "                             full_Train = fullTrain, transform=transform_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "\n",
    "# All training\n",
    "else:\n",
    "    dataset = MyDataset(csv_dir, root_dir, is_Train=True, full_Train = fullTrain, transform=transform_train)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a839e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = int(random.random()*len(train_dataset))\n",
    "# # idx = 1637\n",
    "# print(idx)\n",
    "\n",
    "# plt.imshow(train_dataset[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26256c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e44cdc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Focal Loss\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=0.8, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e42e6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss()\n",
    "optimizer = AdamW(net.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731a3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e328e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 2.7585 | Time(iter) 1.1238\n",
      "Iter [ 40/148] | Train Loss 0.7177 | Time(iter) 0.5200\n",
      "Iter [ 80/148] | Train Loss 0.3354 | Time(iter) 0.5198\n",
      "Iter [120/148] | Train Loss 0.3137 | Time(iter) 0.5194\n",
      "Iter [147/148] | Train Loss 0.2452 | Time(iter) 0.3618\n",
      "BEST in epoch 1 | Acc(or train loss) 0.2452\n",
      "Epoch 2/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.2367 | Time(iter) 0.5228\n",
      "Iter [ 40/148] | Train Loss 0.3084 | Time(iter) 0.5209\n",
      "Iter [ 80/148] | Train Loss 0.1702 | Time(iter) 0.5203\n",
      "Iter [120/148] | Train Loss 0.1821 | Time(iter) 0.5195\n",
      "Iter [147/148] | Train Loss 0.0699 | Time(iter) 0.3607\n",
      "BEST in epoch 2 | Acc(or train loss) 0.0699\n",
      "Epoch 3/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.1289 | Time(iter) 0.5232\n",
      "Iter [ 40/148] | Train Loss 0.0769 | Time(iter) 0.5210\n",
      "Iter [ 80/148] | Train Loss 0.0778 | Time(iter) 0.5219\n",
      "Iter [120/148] | Train Loss 0.1134 | Time(iter) 0.5191\n",
      "Iter [147/148] | Train Loss 0.0321 | Time(iter) 0.3595\n",
      "BEST in epoch 3 | Acc(or train loss) 0.0321\n",
      "Epoch 4/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.1238 | Time(iter) 0.5225\n",
      "Iter [ 40/148] | Train Loss 0.0520 | Time(iter) 0.5192\n",
      "Iter [ 80/148] | Train Loss 0.0495 | Time(iter) 0.5205\n",
      "Iter [120/148] | Train Loss 0.0751 | Time(iter) 0.5194\n",
      "Iter [147/148] | Train Loss 0.0836 | Time(iter) 0.3598\n",
      "BEST in epoch 4 | Acc(or train loss) 0.0495\n",
      "Epoch 5/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0796 | Time(iter) 0.5235\n",
      "Iter [ 40/148] | Train Loss 0.0535 | Time(iter) 0.5205\n",
      "Iter [ 80/148] | Train Loss 0.0794 | Time(iter) 0.5398\n",
      "Iter [120/148] | Train Loss 0.0571 | Time(iter) 0.5198\n",
      "Iter [147/148] | Train Loss 0.0388 | Time(iter) 0.3597\n",
      "BEST in epoch 5 | Acc(or train loss) 0.0388\n",
      "Epoch 6/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0380 | Time(iter) 0.5231\n",
      "Iter [ 40/148] | Train Loss 0.0364 | Time(iter) 0.5199\n",
      "Iter [ 80/148] | Train Loss 0.0794 | Time(iter) 0.5201\n",
      "Iter [120/148] | Train Loss 0.0449 | Time(iter) 0.5207\n",
      "Iter [147/148] | Train Loss 0.0769 | Time(iter) 0.3599\n",
      "BEST in epoch 6 | Acc(or train loss) 0.0364\n",
      "Epoch 7/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0425 | Time(iter) 0.5236\n",
      "Iter [ 40/148] | Train Loss 0.0271 | Time(iter) 0.5196\n",
      "Iter [ 80/148] | Train Loss 0.0314 | Time(iter) 0.5202\n",
      "Iter [120/148] | Train Loss 0.0438 | Time(iter) 0.5190\n",
      "Iter [147/148] | Train Loss 0.0991 | Time(iter) 0.3592\n",
      "BEST in epoch 7 | Acc(or train loss) 0.0271\n",
      "Epoch 8/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0294 | Time(iter) 0.5238\n",
      "Iter [ 40/148] | Train Loss 0.0416 | Time(iter) 0.5198\n",
      "Iter [ 80/148] | Train Loss 0.0383 | Time(iter) 0.5197\n",
      "Iter [120/148] | Train Loss 0.0500 | Time(iter) 0.5199\n",
      "Iter [147/148] | Train Loss 0.0197 | Time(iter) 0.3601\n",
      "BEST in epoch 8 | Acc(or train loss) 0.0197\n",
      "Epoch 9/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0431 | Time(iter) 0.5219\n",
      "Iter [ 40/148] | Train Loss 0.0525 | Time(iter) 0.5197\n",
      "Iter [ 80/148] | Train Loss 0.0444 | Time(iter) 0.5192\n",
      "Iter [120/148] | Train Loss 0.0407 | Time(iter) 0.5208\n",
      "Iter [147/148] | Train Loss 0.0915 | Time(iter) 0.3596\n",
      "BEST in epoch 9 | Acc(or train loss) 0.0407\n",
      "Epoch 10/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0367 | Time(iter) 0.5223\n",
      "Iter [ 40/148] | Train Loss 0.0290 | Time(iter) 0.5198\n",
      "Iter [ 80/148] | Train Loss 0.0251 | Time(iter) 0.5207\n",
      "Iter [120/148] | Train Loss 0.0419 | Time(iter) 0.5196\n",
      "Iter [147/148] | Train Loss 0.0519 | Time(iter) 0.3600\n",
      "BEST in epoch 10 | Acc(or train loss) 0.0251\n",
      "Epoch 11/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0237 | Time(iter) 0.5247\n",
      "Iter [ 40/148] | Train Loss 0.0594 | Time(iter) 0.5356\n",
      "Iter [ 80/148] | Train Loss 0.0366 | Time(iter) 0.5205\n",
      "Iter [120/148] | Train Loss 0.0382 | Time(iter) 0.5193\n",
      "Iter [147/148] | Train Loss 0.0235 | Time(iter) 0.3598\n",
      "BEST in epoch 11 | Acc(or train loss) 0.0235\n",
      "Epoch 12/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0559 | Time(iter) 0.5296\n",
      "Iter [ 40/148] | Train Loss 0.0536 | Time(iter) 0.5199\n",
      "Iter [ 80/148] | Train Loss 0.0425 | Time(iter) 0.5202\n",
      "Iter [120/148] | Train Loss 0.0297 | Time(iter) 0.5200\n",
      "Iter [147/148] | Train Loss 0.0386 | Time(iter) 0.3605\n",
      "BEST in epoch 12 | Acc(or train loss) 0.0297\n",
      "Epoch 13/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0227 | Time(iter) 0.5232\n",
      "Iter [ 40/148] | Train Loss 0.0402 | Time(iter) 0.5199\n",
      "Iter [ 80/148] | Train Loss 0.0388 | Time(iter) 0.5195\n",
      "Iter [120/148] | Train Loss 0.0207 | Time(iter) 0.5196\n",
      "Iter [147/148] | Train Loss 0.0403 | Time(iter) 0.3594\n",
      "BEST in epoch 13 | Acc(or train loss) 0.0207\n",
      "Epoch 14/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0267 | Time(iter) 0.5238\n",
      "Iter [ 40/148] | Train Loss 0.0485 | Time(iter) 0.5197\n",
      "Iter [ 80/148] | Train Loss 0.0512 | Time(iter) 0.5208\n",
      "Iter [120/148] | Train Loss 0.0319 | Time(iter) 0.5200\n",
      "Iter [147/148] | Train Loss 0.0345 | Time(iter) 0.3611\n",
      "BEST in epoch 14 | Acc(or train loss) 0.0267\n",
      "Epoch 15/15\n",
      "============\n",
      "Iter [  0/148] | Train Loss 0.0336 | Time(iter) 0.5429\n",
      "Iter [ 40/148] | Train Loss 0.0257 | Time(iter) 0.5212\n",
      "Iter [ 80/148] | Train Loss 0.0424 | Time(iter) 0.5193\n",
      "Iter [120/148] | Train Loss 0.0258 | Time(iter) 0.5211\n",
      "Iter [147/148] | Train Loss 0.0728 | Time(iter) 0.3590\n",
      "BEST in epoch 15 | Acc(or train loss) 0.0257\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "net.cuda()\n",
    "# best_acc = 0.0\n",
    "os.makedirs('model', exist_ok=True)\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('='*12)\n",
    "    # Train\n",
    "    bestResult = 10000\n",
    "    for iter, (img, label) in enumerate(train_loader):\n",
    "        start = time.time()\n",
    "        \n",
    "        # forward propagation\n",
    "        optimizer.zero_grad()\n",
    "        img, label = img.type(torch.FloatTensor).cuda(), label.cuda()\n",
    "        pred_logit = net(img)\n",
    "        loss = criterion(pred_logit, label)\n",
    "        \n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        # Check\n",
    "        if (iter % check_period == 0) or (iter == len(train_loader)-1):\n",
    "            \n",
    "            # Validation            \n",
    "            if not fullTrain:\n",
    "                valid_loss, valid_acc = AverageMeter(), AverageMeter()\n",
    "\n",
    "                for img, label in val_loader:\n",
    "                    img, label = img.type(torch.FloatTensor).cuda(), label.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        pred_logit = net(img)\n",
    "                    loss = criterion(pred_logit, label)\n",
    "\n",
    "                    # Accuracy 계산\n",
    "                    pred_label = torch.max(pred_logit, 1)[1]\n",
    "                    acc = (pred_label == label).sum() / np.float(len(img))\n",
    "\n",
    "                    valid_loss.update(loss.item(), len(img))\n",
    "                    valid_acc.update(acc, len(img))\n",
    "\n",
    "                valid_loss = valid_loss.avg\n",
    "                valid_acc = valid_acc.avg\n",
    "\n",
    "                print(\"Iter [%3d/%3d] | Train Loss %.4f | Valid Loss %.4f | Valid Acc %.4f | Time(iter) %.4f\"  %\n",
    "                    (iter, len(train_loader), train_loss, valid_loss, valid_acc, end - start))\n",
    "                if valid_acc > bestResult:\n",
    "                    bestResult = valid_acc\n",
    "                    best_model_weights = copy.deepcopy(net.state_dict())\n",
    "                    \n",
    "            # Full data training check    \n",
    "            else:\n",
    "                if train_loss < bestResult:\n",
    "                    bestResult = train_loss\n",
    "                    best_model_weights = copy.deepcopy(net.state_dict())\n",
    "                    \n",
    "                print(\"Iter [%3d/%3d] | Train Loss %.4f | Time(iter) %.4f\" %\n",
    "                    (iter, len(train_loader), train_loss, end-start))\n",
    "        # Check\n",
    "    # Train\n",
    "    if epoch < epochs:\n",
    "        scheduler.step()\n",
    "        \n",
    "    torch.save(best_model_weights, f'model/efficientnet_{model_name}_epoch{epoch + 1}.pth')\n",
    "    print(\"BEST in epoch {} | Acc(or train loss) {:.4f}\".format(epoch + 1, bestResult))\n",
    "    \n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e832",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c439ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'input/data/eval'\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         image = Image.open(self.img_paths[index])\n",
    "        \n",
    "        # Albumentations\n",
    "        image = cv2.imread(self.img_paths[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        # Albumentations\n",
    "        \n",
    "        # Ori\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        # Ori\n",
    "        return image\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61c5b54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "save_epoch = 4\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "dataset = TestDataset(image_paths, transform_val)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(f'model/efficientnet_{model_name}_epoch{save_epoch}.pth'))\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for img in loader:\n",
    "    img = img.type(torch.FloatTensor).cuda()\n",
    "    with torch.no_grad():\n",
    "        pred_logit = model(img)\n",
    "        pred = pred_logit.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "# torch.save(best_model_weights, f'model/efficientnet_{model_name}_epoch{epoch + 1}.pth')\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, f'{model_name}_epoch{save_epoch}.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10bd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111429a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
