{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bc5d80-9ed2-4eae-abc2-6f7ce96c6b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU availalbe : True\n",
      "1 GPU(s) is(are) allocated\n"
     ]
    }
   ],
   "source": [
    "# GPU 할당확인 \n",
    "import torch\n",
    "\n",
    "print('CUDA GPU availalbe : {}'.format(torch.cuda.is_available()))\n",
    "try:\n",
    "    print('{} GPU(s) is(are) allocated'.format(torch.cuda.device_count()))\n",
    "except:\n",
    "    print('GPUs are not allocated. Current runtime is on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2df7557-69cf-425c-b44a-621fc4a72b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9836f18-7857-44ed-b9f5-10c13e83e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = 'input/data/train/train.csv'\n",
    "train_dir = 'input/data/train'\n",
    "root_dir = 'input/data/train/images/'\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, is_Train=True, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv 파일의 경로\n",
    "            img_dir (string): 모든 이미지가 존재하는 디렉토리 경로\n",
    "            transform (callable, optional): 샘플에 적용될 Optional transform\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_path = []\n",
    "        path = csv['path']\n",
    "        \n",
    "        for person in path:\n",
    "            images = [root_dir + person + '/' + image for image in os.listdir(root_dir + person) if not image[:1] == '.']\n",
    "            for image in images:\n",
    "                self.img_path.append(image)\n",
    "\n",
    "        self.comb_dic = {}\n",
    "        comb = [(m, g, a) for m in ['m', 'i', 'n'] for g in ['male', 'female'] for a in [0, 1, 2]]\n",
    "        for i, (m, g, a) in enumerate(comb):\n",
    "            self.comb_dic[(m, g, a)] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_path[idx]\n",
    "\n",
    "        mask = img_name.split('/')[-1][:1]\n",
    "        features = img_name.split('/')[-2].split('_')\n",
    "        gender = features[1]\n",
    "        if features[3] in ['58', '59']:\n",
    "            age = 2\n",
    "        else:\n",
    "            age = int(features[3]) // 30\n",
    "\n",
    "        target = self.comb_dic[(mask, gender, age)]\n",
    "        \n",
    "        img = Image.open(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f466a4-75e3-4986-8df1-5adebb22dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=18)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff3edc9-a5bf-4eb9-8fe3-1b8e3ba55aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda3de20-d649-4525-b934-d310dcfdbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8aecb4-e62c-4195-abbf-092855048a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.Grayscale(num_output_channels=3),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     torchvision.transforms.CenterCrop((450, 360)),\n",
    "    #transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.Resize((200,200), Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    #transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((200,200), Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# random_split\n",
    "# train_dataset = MyDataset(csv_dir, root_dir, is_Train=True, transform=transform_train)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "#                           num_workers=1, shuffle=True) #sampler = sampler)\n",
    "\n",
    "# valid_dataset = MyDataset(csv_dir, root_dir, is_Train=False, transform=transform_val)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True, num_workers=1)\n",
    "\n",
    "# sampler split\n",
    "dataset = MyDataset(csv_dir, root_dir, is_Train=True, transform=transform_train)\n",
    "\n",
    "val_idx = []\n",
    "person_idx = list(range(2700))\n",
    "np.random.seed(211)\n",
    "np.random.shuffle(person_idx)\n",
    "n_val_person = int(2700 * 0.2)\n",
    "val_person_idx = person_idx[:n_val_person]\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "for i in range(2700*7):\n",
    "    if i // 7 not in val_person_idx:\n",
    "        train_indices.append(i)\n",
    "    else:\n",
    "        val_indices.append(i)\n",
    "\n",
    "\n",
    "# n_val = int(len(dataset) * 0.2)\n",
    "# n_train = len(dataset) - n_val\n",
    "# train_dataset, valid_dataset = random_split(dataset, [len(dataset),0]) # comment when split by person\n",
    "# train_indices = list(range(n_train))\n",
    "# np.random.shuffle(train_indices)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4, sampler=train_indices)\n",
    "valid_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4, sampler=val_indices)\n",
    "\n",
    "# # SubsetRandomSampler split\n",
    "# dataset = MyDataset(csv_dir, root_dir, is_Train=True, transform=transform_train)\n",
    "\n",
    "# validation_split = 0.2\n",
    "# shuffle_dataset = True\n",
    "# random_seed = 42\n",
    "# dataset_size = len(dataset)\n",
    "# indices = list(range(dataset_size))\n",
    "# split = int(np.floor(validation_split * dataset_size))\n",
    "# if shuffle_dataset:\n",
    "#     np.random.seed(random_seed)\n",
    "#     np.random.shuffle(indices)\n",
    "# train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# # train_dataset, valid_dataset = random_split(dataset, [len(dataset),0]) # comment when split by person\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4, sampler=train_sampler)\n",
    "# valid_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828b1f2a-a567-4bdb-a516-4328b38b9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3cd328-9206-4d93-9dcb-95823e214d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Focal Loss\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=0.8, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaeb8858-b24a-43a6-a9d8-2f0d2d702488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss()\n",
    "optimizer = AdamW(net.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd45256b-a9ac-49b7-bdc8-bfc5ee745c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4ef26b-4d0f-443a-ae1e-4cfcf2cf9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('model/final_experiment4')\n",
    "model_name = \"agefilter_focal_noval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fe58a-2377-4264-a466-a8e91ad3f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 2.7840 | Valid Loss 2.7552 | Valid Acc 0.0706\n",
      "Iter [ 20/119] | Train Loss 2.9889 | Valid Loss 2.3982 | Valid Acc 0.2471\n",
      "Iter [ 40/119] | Train Loss 1.6138 | Valid Loss 1.4591 | Valid Acc 0.4119\n",
      "Iter [ 60/119] | Train Loss 3.3728 | Valid Loss 2.3437 | Valid Acc 0.2884\n",
      "Iter [ 80/119] | Train Loss 1.2709 | Valid Loss 1.8966 | Valid Acc 0.2511\n",
      "Iter [100/119] | Train Loss 1.6714 | Valid Loss 1.2813 | Valid Acc 0.4175\n",
      "Iter [118/119] | Train Loss 0.5030 | Valid Loss 1.7793 | Valid Acc 0.3558\n",
      "BEST in epoch 1 | Acc 0.4175\n",
      "Epoch 2/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 3.0879 | Valid Loss 1.8208 | Valid Acc 0.3370\n",
      "Iter [ 20/119] | Train Loss 1.2426 | Valid Loss 1.3097 | Valid Acc 0.3601\n",
      "Iter [ 40/119] | Train Loss 1.0720 | Valid Loss 0.9697 | Valid Acc 0.5376\n",
      "Iter [ 60/119] | Train Loss 1.7810 | Valid Loss 1.3439 | Valid Acc 0.4153\n",
      "Iter [ 80/119] | Train Loss 0.7012 | Valid Loss 1.3166 | Valid Acc 0.4108\n",
      "Iter [100/119] | Train Loss 1.2238 | Valid Loss 0.9743 | Valid Acc 0.5214\n",
      "Iter [118/119] | Train Loss 0.3020 | Valid Loss 1.3070 | Valid Acc 0.4481\n",
      "BEST in epoch 2 | Acc 0.5376\n",
      "Epoch 3/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 2.1466 | Valid Loss 1.3265 | Valid Acc 0.4466\n",
      "Iter [ 20/119] | Train Loss 0.2851 | Valid Loss 0.8571 | Valid Acc 0.6307\n",
      "Iter [ 40/119] | Train Loss 0.5005 | Valid Loss 0.7309 | Valid Acc 0.6627\n",
      "Iter [ 60/119] | Train Loss 0.6368 | Valid Loss 0.6872 | Valid Acc 0.6775\n",
      "Iter [ 80/119] | Train Loss 0.5784 | Valid Loss 0.6792 | Valid Acc 0.6778\n",
      "Iter [100/119] | Train Loss 0.6891 | Valid Loss 0.6941 | Valid Acc 0.6606\n",
      "Iter [118/119] | Train Loss 0.4588 | Valid Loss 0.6437 | Valid Acc 0.6944\n",
      "BEST in epoch 3 | Acc 0.6944\n",
      "Epoch 4/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 0.8100 | Valid Loss 0.6513 | Valid Acc 0.6971\n",
      "Iter [ 20/119] | Train Loss 0.1210 | Valid Loss 0.6499 | Valid Acc 0.6966\n",
      "Iter [ 40/119] | Train Loss 0.4217 | Valid Loss 0.6325 | Valid Acc 0.7045\n",
      "Iter [ 60/119] | Train Loss 0.5320 | Valid Loss 0.6278 | Valid Acc 0.7101\n",
      "Iter [ 80/119] | Train Loss 0.3554 | Valid Loss 0.5837 | Valid Acc 0.7204\n",
      "Iter [100/119] | Train Loss 0.3234 | Valid Loss 0.6591 | Valid Acc 0.6786\n",
      "Iter [118/119] | Train Loss 0.1519 | Valid Loss 0.5692 | Valid Acc 0.7225\n",
      "BEST in epoch 4 | Acc 0.7225\n",
      "Epoch 5/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 0.5353 | Valid Loss 0.5736 | Valid Acc 0.7272\n",
      "Iter [ 20/119] | Train Loss 0.0811 | Valid Loss 0.5633 | Valid Acc 0.7378\n",
      "Iter [ 40/119] | Train Loss 0.4299 | Valid Loss 0.5519 | Valid Acc 0.7399\n",
      "Iter [ 60/119] | Train Loss 0.3143 | Valid Loss 0.5473 | Valid Acc 0.7439\n",
      "Iter [ 80/119] | Train Loss 0.2062 | Valid Loss 0.5400 | Valid Acc 0.7437\n",
      "Iter [100/119] | Train Loss 0.1520 | Valid Loss 0.5400 | Valid Acc 0.7426\n",
      "Iter [118/119] | Train Loss 0.1480 | Valid Loss 0.5331 | Valid Acc 0.7442\n",
      "BEST in epoch 5 | Acc 0.7442\n",
      "Epoch 6/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 0.4994 | Valid Loss 0.5364 | Valid Acc 0.7405\n",
      "Iter [ 20/119] | Train Loss 0.0812 | Valid Loss 0.5315 | Valid Acc 0.7505\n",
      "Iter [ 40/119] | Train Loss 0.3382 | Valid Loss 0.5327 | Valid Acc 0.7495\n",
      "Iter [ 60/119] | Train Loss 0.2878 | Valid Loss 0.5379 | Valid Acc 0.7397\n",
      "Iter [ 80/119] | Train Loss 0.2311 | Valid Loss 0.5292 | Valid Acc 0.7468\n",
      "Iter [100/119] | Train Loss 0.1485 | Valid Loss 0.5254 | Valid Acc 0.7489\n",
      "Iter [118/119] | Train Loss 0.1393 | Valid Loss 0.5199 | Valid Acc 0.7513\n",
      "BEST in epoch 6 | Acc 0.7513\n",
      "Epoch 7/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 0.4574 | Valid Loss 0.5233 | Valid Acc 0.7471\n",
      "Iter [ 20/119] | Train Loss 0.0842 | Valid Loss 0.5227 | Valid Acc 0.7471\n",
      "Iter [ 40/119] | Train Loss 0.3133 | Valid Loss 0.5226 | Valid Acc 0.7537\n",
      "Iter [ 60/119] | Train Loss 0.2254 | Valid Loss 0.5194 | Valid Acc 0.7532\n",
      "Iter [ 80/119] | Train Loss 0.1957 | Valid Loss 0.5217 | Valid Acc 0.7542\n",
      "Iter [100/119] | Train Loss 0.1245 | Valid Loss 0.5224 | Valid Acc 0.7476\n",
      "Iter [118/119] | Train Loss 0.1317 | Valid Loss 0.5264 | Valid Acc 0.7526\n",
      "BEST in epoch 7 | Acc 0.7542\n",
      "Epoch 8/20\n",
      "============\n",
      "Iter [  0/119] | Train Loss 0.4570 | Valid Loss 0.5231 | Valid Acc 0.7505\n",
      "Iter [ 20/119] | Train Loss 0.0708 | Valid Loss 0.5249 | Valid Acc 0.7489\n",
      "Iter [ 40/119] | Train Loss 0.2981 | Valid Loss 0.5149 | Valid Acc 0.7500\n",
      "Iter [ 60/119] | Train Loss 0.2436 | Valid Loss 0.5135 | Valid Acc 0.7548\n",
      "Iter [ 80/119] | Train Loss 0.1608 | Valid Loss 0.5136 | Valid Acc 0.7545\n",
      "Iter [100/119] | Train Loss 0.1163 | Valid Loss 0.5198 | Valid Acc 0.7511\n"
     ]
    }
   ],
   "source": [
    "net.cuda()\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('='*12)\n",
    "    \n",
    "    # Train\n",
    "    for iter, (img, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        img, label = img.type(torch.FloatTensor).cuda(), label.cuda()\n",
    "        pred_logit = net(img)\n",
    "        loss = criterion(pred_logit, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "  \n",
    "        # Validation\n",
    "        if (iter % 20 == 0) or (iter == len(train_loader)-1):\n",
    "# val\n",
    "            valid_loss, valid_acc = AverageMeter(), AverageMeter()\n",
    "            \n",
    "            #print(\"   pred: \", pred_logit.argmax(dim=-1)[:8])\n",
    "            #print(\"   label:\", label[:8])\n",
    "\n",
    "            for img, label in valid_loader:\n",
    "                img, label = img.type(torch.FloatTensor).cuda(), label.cuda()\n",
    "                with torch.no_grad():\n",
    "                    pred_logit = net(img)\n",
    "                loss = criterion(pred_logit, label)\n",
    "\n",
    "                # Accuracy 계산\n",
    "                pred_label = torch.max(pred_logit, 1)[1]\n",
    "                acc = (pred_label == label).sum() / np.float(len(img))\n",
    "                valid_loss.update(loss.item(), len(img))\n",
    "                valid_acc.update(acc, len(img))\n",
    "\n",
    "            valid_loss = valid_loss.avg\n",
    "            valid_acc = valid_acc.avg\n",
    "\n",
    "            print(\"Iter [%3d/%3d] | Train Loss %.4f | Valid Loss %.4f | Valid Acc %.4f\" %\n",
    "                (iter, len(train_loader), train_loss, valid_loss, valid_acc))\n",
    "# val\n",
    "            \n",
    "#             print(\"Iter [%3d/%3d] | Train Loss %.4f \" %\n",
    "#                 (iter, len(train_loader), train_loss))\n",
    "\n",
    "            writer.add_scalar('training loss', train_loss, epoch * len(train_loader) + iter)\n",
    "# val\n",
    "            writer.add_scalar('validation loss', valid_loss, epoch * len(train_loader) + iter)\n",
    "            writer.add_scalar('validation acc', valid_acc, epoch * len(train_loader) + iter)\n",
    "      \n",
    "            if valid_acc > best_acc:\n",
    "                best_acc = valid_acc\n",
    "                best_model_weights = copy.deepcopy(net.state_dict())\n",
    "# val             \n",
    "            best_model_weights = copy.deepcopy(net.state_dict())\n",
    "            \n",
    "    if epoch < epochs:\n",
    "        scheduler.step()\n",
    "    \n",
    "    torch.save(best_model_weights, f'model/efficientnet_{model_name}_epoch{epoch + 1}.pth')\n",
    "    print(\"BEST in epoch {} | Acc {:.4f}\".format(epoch + 1, best_acc))\n",
    "    \n",
    "    \n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e307e3-541c-4c71-8db7-5a4e14b65ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir = 'input/data/eval'\n",
    "\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, img_paths, transform):\n",
    "#         self.img_paths = img_paths\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         image = Image.open(self.img_paths[index])\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fca417-8b5e-4da9-b8b5-704c7e74175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meta 데이터와 이미지 경로를 불러옵니다.\n",
    "# submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "# image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# # Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "# image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "# dataset = TestDataset(image_paths, transform_val)\n",
    "\n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=256,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# # 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('model/efficientnet_agefilter_focal_noval_epoch2.pth'))\n",
    "# model.eval()\n",
    "# model.cuda()\n",
    "\n",
    "# # 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "# all_predictions = []\n",
    "# for img in loader:\n",
    "#     img = img.type(torch.FloatTensor).cuda()\n",
    "#     with torch.no_grad():\n",
    "#         pred_logit = model(img)\n",
    "#         pred = pred_logit.argmax(dim=-1)\n",
    "#         all_predictions.extend(pred.cpu().numpy())\n",
    "# submission['ans'] = all_predictions\n",
    "\n",
    "# # 제출할 파일을 저장합니다.\n",
    "# submission.to_csv(os.path.join(test_dir, '4th_10percent_valset.csv'), index=False)\n",
    "# print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570529c-0fb9-45e2-a58f-5ce8c8a547c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
